---
layout: post
title: Dimension reduction with sci-kit learn
comments: true
---

# 2018 Spring Data Analytics @Dept. of Industrial engineering
## Dimension reduction
### Contents
 - Principal component analysis (PCA)
 - Truncated singular value decomposition and latent semantic analysis
 - Non-negative matrix factorization (NMF or NNMF)
 - Latent Dirichlet Allocation (LDA)
 - Another dimension reduction method for Visualization

### Used library
 - Sci-kit learn: Machine learning을 Python에서 손쉽게 이용할 수 있도록 작성된 라이브러리, 전처리/모형구축/평가 등 전 과정에 관련한 모듈등이 구축되어 있음 (http://scikit-learn.org)
 - Matplotlib: Python에서 Plot을 그릴때 가징 기본적인 라이브러리, 본 예제에서는 PCA등을 이용하여 차원축소된 데이터를 시각화 하는데 사용 (https://matplotlib.org/)


```python
import matplotlib.pyplot as plt 
```

### Sample data
샘플데이터는 Sci-kit learn에 포함되어있는 20 Newsgroups data를 사용
 - 20 Newsgroups data: The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups (http://qwone.com/~jason/20Newsgroups/). 


```python
from sklearn.datasets import fetch_20newsgroups

news = fetch_20newsgroups(shuffle=True, random_state=777, remove=('headers', 'footers', 'quotes'))
print('-'*20 + '20 Newsgroups data class' + '-'*20)
print(news.target_names)
print('-'*20 + '20 Newsgroups data sample' + '-'*20)
print(news.data[0])
```

    --------------------20 Newsgroups data class--------------------
    ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']
    --------------------20 Newsgroups data sample--------------------
    
    
    And I recommend the movie _The Thin Blue Line_, which is about the
    same case.  Not as much legal detail, but still an excellent film.  It
    shows how very easy it is to come up with seemingly conclusive
    evidence against someone whom you think is guilty.
    

### Pre-processing


```python
from sklearn.feature_extraction.text import TfidfVectorizer

data_samples = news.data[:1000]
data_target = news.target[:1000]
data_class = news.target_names
tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=100, stop_words='english')
TFIDF = tfidf_vectorizer.fit_transform(data_samples)
```


```python
print(TFIDF.toarray()[0:2])
```

    [[0.         0.         0.         0.         0.         0.
      0.         0.         0.         0.61061632 0.         0.
      0.61941575 0.         0.         0.         0.         0.
      0.         0.         0.         0.         0.         0.
      0.         0.         0.         0.         0.         0.
      0.         0.         0.         0.         0.         0.
      0.         0.         0.         0.         0.         0.
      0.         0.         0.         0.         0.         0.
      0.         0.         0.         0.         0.         0.
      0.         0.         0.         0.         0.         0.
      0.         0.         0.         0.         0.         0.
      0.         0.         0.         0.         0.         0.
      0.         0.         0.         0.         0.         0.
      0.         0.         0.         0.         0.         0.
      0.49342866 0.         0.         0.         0.         0.
      0.         0.         0.         0.         0.         0.
      0.         0.         0.         0.        ]
     [0.         0.         0.         0.         0.         0.
      0.34064892 0.         0.         0.         0.77129062 0.
      0.         0.         0.         0.         0.         0.
      0.         0.         0.         0.         0.         0.
      0.         0.         0.         0.         0.         0.
      0.         0.         0.         0.         0.         0.
      0.         0.31703364 0.         0.         0.         0.
      0.         0.         0.         0.         0.         0.
      0.         0.         0.         0.         0.32037793 0.
      0.         0.         0.         0.         0.         0.
      0.         0.         0.         0.         0.         0.
      0.         0.         0.         0.         0.         0.
      0.         0.         0.         0.         0.         0.
      0.         0.         0.         0.29311558 0.         0.
      0.         0.         0.         0.         0.         0.
      0.         0.         0.         0.         0.         0.
      0.         0.         0.         0.        ]]
    

### Principal component analysis (PCA)


```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)

pca.fit(TFIDF.toarray())
print('-'*20 + 'Explained variance ratio' + '-'*20)
print(pca.explained_variance_ratio_)
print('-'*20 + 'Singular value' + '-'*20)
print(pca.singular_values_)
```

    --------------------Explained variance ratio--------------------
    [0.03153768 0.02615567]
    --------------------Singular value--------------------
    [5.1851592  4.72204602]
    


```python
import matplotlib.cm as cm
import numpy as np

PCA_TFIDF = pca.transform(TFIDF.toarray())
print(PCA_TFIDF.shape)

plt.figure(figsize=(10,10))
plt.scatter(PCA_TFIDF[:,0], PCA_TFIDF[:,1], c=data_target)
plt.show()
```

    (1000, 2)
    


![png](/img/post_img/2018-04-13-dimensionreduction/output_9_1.png)


#### Best example of PCA

Iris data set(https://en.wikipedia.org/wiki/Iris_flower_data_set)
 - Feature: the length and the width of the sepals and petals (4 features)  
 - Class: Iris setosa, Iris virginica and Iris versicolor


```python
from sklearn.datasets import load_iris

IRIS = load_iris()
X = IRIS.data
y = IRIS.target
target_names = IRIS.target_names

pca_iris = PCA(n_components=2)
PCA_IRIS = pca_iris.fit(X).transform(X)

print('-'*20 + 'Explained variance ratio' + '-'*20)
print(pca_iris.explained_variance_ratio_)
print('-'*20 + 'Singular value' + '-'*20)
print(pca_iris.singular_values_)

plt.figure(figsize=(10,10))
plt.scatter(PCA_IRIS[:,0], PCA_IRIS[:,1], c=y)
plt.title('PCA of IRIS dataset')
plt.show()
```

    --------------------Explained variance ratio--------------------
    [0.92461621 0.05301557]
    --------------------Singular value--------------------
    [25.08986398  6.00785254]
    


![png](/img/post_img/2018-04-13-dimensionreduction/output_11_1.png)


### Singular value decomposition (SVD) and Latent semantic analysis (LSA)


```python
from sklearn.decomposition import TruncatedSVD

svd = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=5, random_state=None, tol=0.0)
SVD_TFIDF = svd.fit(TFIDF)

print("Topics in SVD(LSA) model:")
feature_names = tfidf_vectorizer.get_feature_names()
for topic_idx, topic in enumerate(svd.components_):
    message = "Topic #%d: " % topic_idx
    message += ", ".join([feature_names[i] for i in topic.argsort()[:-10 -1:-1]])
    print(message)
```

    Topics in SVD(LSA) model:
    Topic #0: just, don, like, know, people, think, does, time, good, use
    Topic #1: thanks, mail, windows, does, know, help, edu, program, used, use
    Topic #2: know, does, people, thanks, god, think, believe, did, say, mean
    Topic #3: just, know, right, mail, edu, way, thanks, does, key, really
    Topic #4: edu, think, good, don, mail, right, let, com, thanks, know
    Topic #5: good, just, ve, seen, time, does, game, know, didn, really
    Topic #6: think, like, don, did, know, need, drive, game, make, windows
    Topic #7: edu, like, god, ve, seen, new, people, things, believe, say
    Topic #8: ve, use, seen, don, know, used, people, got, just, case
    Topic #9: com, ve, drive, think, thanks, god, seen, did, used, bit
    


```python
print(svd.components_.shape)
print(TFIDF.shape)
```

    (10, 100)
    (1000, 100)
    

#### Image compression with SVD (with numpy)


```python
import numpy as np
from PIL import Image

# 이미지 로딩
img = Image.open("sorry_nephew.jpg")
imggray = img.convert('LA')

# 이미지 벡터화 
imgmat = np.array(list(imggray.getdata(band=0)), float)
imgmat.shape = (imggray.size[1], imggray.size[0])
imgmat = np.matrix(imgmat)

# 원본이미지 출력
plt.figure(figsize=(5,5))
fig_ori=plt.imshow(imgmat, cmap='gray')
fig_ori.axes.get_xaxis().set_visible(False)
fig_ori.axes.get_yaxis().set_visible(False)
```


![png](/img/post_img/2018-04-13-dimensionreduction/output_16_0.png)



```python
# SVD
U, sigma, V = np.linalg.svd(imgmat)

# 특이값 개수별 이미지 재구성 
plt.figure(figsize=(10,10))
for i in range(1, 10):
    reconstimg = np.matrix(U[:, :i*5]) * np.diag(sigma[:i*5]) * np.matrix(V[:i*5, :])
    plt.subplot("33{i}".format(i=i))
    fig = plt.imshow(reconstimg, cmap='gray')
    fig.axes.get_xaxis().set_visible(False)
    fig.axes.get_yaxis().set_visible(False)
    plt.title("n = {i}".format(i=(i*5)))
plt.show()
```


![png](/img/post_img/2018-04-13-dimensionreduction/output_17_0.png)


### Non-negative matrix factorization (NMF)


```python
from sklearn.decomposition import NMF

nmf = NMF(n_components=10, random_state=1, beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1, l1_ratio=.5)
NMT_TFIDF = nmf.fit(TFIDF)

print("Topics in NMF model (generalized Kullback-Leibler divergence):")
feature_names = tfidf_vectorizer.get_feature_names()
for topic_idx, topic in enumerate(nmf.components_):
    message = "Topic #%d: " % topic_idx
    message += ", ".join([feature_names[i] for i in topic.argsort()[:-10 -1:-1]])
    print(message)
```

    Topics in NMF model (generalized Kullback-Leibler divergence):
    Topic #0: make, say, like, way, don, want, does, true, really, problem
    Topic #1: thanks, does, mail, help, windows, know, program, file, need, set
    Topic #2: new, year, probably, key, need, line, years, 10, number, information
    Topic #3: just, right, know, way, mean, really, tell, used, doesn, mail
    Topic #4: think, don, edu, did, want, public, let, going, know, case
    Topic #5: good, time, didn, better, point, game, long, great, just, order
    Topic #6: people, time, believe, god, fact, come, government, world, point, read
    Topic #7: like, come, question, number, different, things, tell, high, believe, seen
    Topic #8: use, ve, used, using, don, available, fact, thing, got, point
    Topic #9: drive, work, seen, com, problems, hard, bit, read, got, ve
    

### Latent Dirichlet Allocation (LDA)


```python
from sklearn.decomposition import LatentDirichletAllocation

lda = LatentDirichletAllocation(n_components=10, random_state=777, verbose=0, learning_method='batch', max_iter=200)
lda.fit(TFIDF)

print("Topics in LDA model:")
feature_names = tfidf_vectorizer.get_feature_names()
for topic_idx, topic in enumerate(lda.components_):
    message = "Topic #%d: " % topic_idx
    message += ", ".join([feature_names[i] for i in topic.argsort()[:-10 -1:-1]])
    print(message)
```

    Topics in LDA model:
    Topic #0: drive, chip, hard, set, believe, problems, look, problem, question, think
    Topic #1: game, time, think, just, right, don, great, like, fact, said
    Topic #2: line, 10, year, did, years, 12, ll, didn, let, dod
    Topic #3: edu, mail, list, make, space, different, thanks, tell, order, word
    Topic #4: good, number, case, like, following, given, government, tape, ve, come
    Topic #5: thanks, does, windows, ve, seen, know, file, like, help, work
    Topic #6: people, god, really, just, don, say, think, know, time, does
    Topic #7: com, key, new, data, information, read, used, just, great, bit
    Topic #8: used, use, program, high, want, son, like, probably, using, help
    Topic #9: public, doesn, going, don, real, mean, way, better, know, problem
    

#### Visualzing the LDA models with pyLDAvis


```python
import pyLDAvis
import pyLDAvis.sklearn

pyLDAvis.enable_notebook()
pyLDAvis.sklearn.prepare(lda, TFIDF, tfidf_vectorizer)
```





<link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css">


<div id="ldavis_el972017619198264643308787605"></div>
<script type="text/javascript">

var ldavis_el972017619198264643308787605_data = {"mdsDat": {"Freq": [26.48039679633146, 16.200628460576148, 9.634163492630787, 9.054740714262334, 6.97115387513996, 6.569305990564693, 6.372639982161964, 6.352846093646139, 6.346599468121568, 6.017525126564949], "cluster": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "topics": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "x": [-0.029354966284783485, -0.09053666330274769, 0.027958423710786957, -0.08958062288738751, 0.07326275632142258, -0.038770691045664454, -0.0006967692248577601, 0.028354832131330047, 0.3303637288495558, -0.211000028267654], "y": [0.06009787192132918, -0.020440967640066417, 0.18178884282699753, 0.12752247315164095, -0.31534130234519914, 0.03610159456996767, -0.03632355321587773, -0.03403808261765913, 0.05879671007613967, -0.05816358672727283]}, "tinfo": {"Category": ["Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10"], "Freq": [34.0, 24.0, 29.0, 38.0, 31.0, 22.0, 22.0, 39.0, 34.0, 20.0, 21.0, 17.0, 20.0, 19.0, 24.0, 18.0, 24.0, 22.0, 17.0, 28.0, 14.0, 21.0, 17.0, 37.0, 36.0, 19.0, 32.0, 55.0, 41.0, 17.0, 19.179124270829497, 18.239637826282642, 4.5874434207718195, 10.501046385448822, 28.246193322920313, 15.117960641935417, 10.28483748872972, 10.891096028939595, 8.159735184938194, 8.479272904843027, 9.455656139741013, 9.593718628033574, 10.211591820470126, 7.007010874606873, 6.026267091853931, 10.888536991221034, 7.891283188734812, 8.481862605749678, 7.815545507482922, 5.964148289489627, 6.903545490713341, 5.28095666086518, 7.33646107666613, 5.166437157498767, 5.985906123977264, 9.514010614757177, 7.48084615816358, 6.278462426861663, 14.173759441074692, 6.307937733545612, 17.94139162666506, 15.570596368715105, 9.461116355776552, 11.632207120423981, 11.205099449768216, 12.8460176379173, 9.166845450735277, 11.134919875910938, 8.809751232359444, 7.889870306936598, 7.345231227564946, 19.759496755088616, 16.987104178006284, 13.756056340860901, 23.81935938796027, 23.042429825440998, 18.199907817512123, 9.761851928577263, 10.603911386182464, 7.950144013393136, 6.6455355739406645, 6.800923683578556, 16.449326668666686, 6.124927244291051, 5.192917316903887, 7.357332304989197, 6.280903622347126, 7.14140195406384, 3.979328637604949, 4.74696333205328, 4.841368796952138, 6.7964971434266195, 4.481992065643772, 3.841275283970969, 2.809756290378185, 11.718719598083045, 3.624471197422378, 3.5319364819741885, 3.273240850278372, 7.232457130000122, 5.242951619432707, 4.318584270559555, 5.165698252470146, 4.910924602072141, 5.450447606768627, 4.040202292668203, 4.090359142351039, 4.082923317957976, 17.591279708701794, 9.68378198210246, 7.423546278987797, 8.226695374778021, 16.297182740794607, 7.781002014229688, 15.793392787550838, 10.602725628851399, 5.466619286132586, 13.92231371461, 5.586381713579993, 5.07501573958374, 5.256217525136304, 9.952581142052512, 3.433123764492942, 4.843278643110174, 4.662677465233422, 8.727823986097118, 1.9764942864759085, 2.5072171808554677, 2.4979360614687836, 5.896274493137124, 3.8648532448899386, 1.7029766510457618, 4.317959369909079, 2.5187538947516055, 2.4541331998770577, 1.8161911086145783, 3.4520155363930587, 1.2534619514286196, 1.599773330889583, 15.69902063880159, 9.749856360255562, 11.168977687930477, 10.923653449489425, 7.566478504398579, 6.404637872481133, 4.651895307150623, 4.413963315260841, 3.7543579000728857, 6.143369129006246, 6.484178467686559, 10.31801683254358, 3.645352993133914, 3.1451861352473984, 3.921370311541983, 3.924685537046133, 4.04182363017455, 1.0933140758996318, 4.324051989745786, 2.6100203816549565, 6.17935642805505, 1.6702837116560048, 2.0577267963288013, 3.938800690672005, 3.2856041380239476, 4.341703036223135, 2.167094465838752, 1.9287372851148112, 2.145230425264491, 1.649221891630605, 5.1182530039696, 3.5103641550156137, 3.590417453004786, 3.6300906577828824, 3.4330084555187717, 3.8118776234879177, 21.80770975487797, 21.637222195424403, 13.634180352613594, 3.3955123087450283, 10.496184039385009, 17.065129419174838, 8.072066830310794, 3.449178207916135, 3.6518491822770014, 2.380759320338165, 4.9918375096766, 2.204859322233995, 2.632811495292542, 1.7267675246071708, 1.7069044083884652, 1.6576906765281365, 2.314030971559883, 2.8373689264772253, 4.735903823200284, 1.437523227025838, 1.24841932962006, 1.1479628380761449, 1.2427045841337507, 0.7546917242125845, 1.8586020115031703, 1.031140932890982, 0.9557366339393213, 0.5429474857295692, 0.2542319222626541, 0.1083204334304885, 0.2404827993376106, 0.38736252476961164, 29.82438102018704, 8.710672297753305, 10.566015723311006, 21.152947284944258, 8.458282181866645, 5.173771792674243, 5.191448784244206, 10.500445691606897, 5.5806871736840495, 6.4599232709387895, 1.4238481343610174, 3.9765119007561553, 2.053849647641899, 1.4061652234210236, 1.2579740780818744, 1.8679869668236637, 1.1605886828922916, 0.8884567590820774, 2.4917876363319627, 0.8813019017704904, 0.8630233247977745, 1.4703224688637286, 0.11059963941658696, 0.510842093797236, 0.11064044854694399, 0.11063896852703359, 0.11061945857084718, 0.11059984267861282, 0.11061922761323259, 0.11061656620392989, 0.11064837493093627, 0.11116623705534608, 0.11080786271834703, 0.11078013764038404, 0.11074991617996789, 0.11066802516687553, 0.11066224048981471, 0.11065613254632041, 23.119123546495217, 16.99270952070354, 12.457756010863974, 11.634320905653984, 10.377818683119944, 6.295050972354502, 6.2976349332293, 4.097471735073957, 4.56871482009689, 3.5208559784701112, 3.682195317733469, 1.6039150991132423, 2.4363730966821704, 3.037755390639902, 1.5242139589600885, 1.6388976142988738, 3.6849549184226547, 1.4480072053591535, 1.5924744688817178, 2.0335037318147102, 1.0163154133024566, 2.0106042660507133, 1.7334775202828916, 0.11634499982194194, 0.5387367657618775, 0.11634286359332961, 0.11634725820709975, 0.11635785472080283, 0.11634283205196842, 0.11639697490532583, 0.11671793316376558, 0.11648403564035437, 0.11644330686434079, 0.1164424858346065, 0.11643581393265034, 0.11642515131089179, 5.169293907152647, 14.161593840419474, 8.683618838465748, 8.574301332116843, 12.936093486661845, 17.178325349487853, 7.188592512839821, 5.077930288501445, 2.277654210973853, 4.13855248380329, 2.873227948291797, 10.32920711720733, 5.084721626825496, 2.2966814186455746, 2.867720577651156, 2.0989849569977626, 3.0288489026026655, 1.8188779000554718, 1.5702802767719557, 1.8704727069748424, 1.275424178130589, 0.8715075478134326, 2.060942672602698, 0.6345322360587523, 0.9052158969743586, 1.469023226045202, 0.11432842746484245, 0.3710868781620503, 0.11433432504382367, 0.11435105252479079, 0.3956033485380408, 0.4364152177465617, 0.11453176044371001, 16.206259740981096, 4.89010758993944, 10.242329904733412, 17.2345757122326, 13.820199573062935, 9.387641715200044, 10.99773929830489, 4.018794438707888, 11.106069478633323, 6.508791774753406, 6.471298245563591, 2.536414273202912, 4.413025447479252, 1.1779084302156757, 2.4308544544334, 1.9948219356940475, 2.010982113907928, 0.11329544813263276, 0.7138287378241918, 0.11329698850885371, 0.11330094880310145, 0.2864821844429937, 0.11329416012932372, 0.11374067608235955, 0.11329669908646434, 0.11331706825438105, 0.11331364693230443, 0.11330365962572539, 0.11644182154868249, 0.11331090764203712, 0.11353799320089394, 0.11346567991284763, 0.11340395655746432, 0.11337669958293056, 0.11337657273776665, 0.11334602768427898, 0.11333944028643424, 0.11333678838841658, 0.1133349639018056, 6.648090540280585, 13.02527113253983, 19.55482429268081, 19.476240031319453, 7.28193115644012, 4.354294245912451, 6.968026639205135, 5.5743973320966145, 5.360909423442886, 5.51042650207239, 3.488392615162708, 2.836232075157884, 2.3343614030058375, 3.176876987598472, 6.01184619026801, 2.024435143570652, 1.288553545753862, 0.9211055563427497, 0.8902485745415847, 1.862871741950454, 0.11552324099835304, 0.42264788460233255, 0.11736985481649785, 0.11556811603700305, 0.11552698524680385, 0.5706794474571245, 0.11557245165899696, 0.11554757249770341, 0.11553223331014613, 0.11553355560171212, 0.11607519326256832, 0.1159813558315372, 0.11579368182557187, 0.11562266006174853, 0.1156211362128514, 0.1156038556170757, 0.11559568203805418], "Term": ["edu", "drive", "mail", "use", "used", "com", "key", "good", "new", "chip", "line", "10", "program", "number", "year", "game", "case", "believe", "hard", "did", "data", "years", "public", "time", "thanks", "set", "make", "like", "think", "information", "god", "really", "armenian", "day", "people", "say", "law", "things", "thing", "try", "world", "come", "government", "order", "word", "believe", "mean", "point", "said", "far", "look", "called", "question", "idea", "given", "way", "let", "fact", "think", "work", "just", "don", "right", "time", "does", "know", "make", "like", "good", "did", "want", "windows", "seen", "file", "thanks", "does", "ve", "work", "help", "got", "available", "problems", "know", "bit", "try", "using", "program", "problem", "following", "question", "let", "mail", "set", "information", "idea", "like", "read", "ll", "better", "use", "need", "say", "good", "time", "don", "want", "make", "new", "game", "great", "true", "fact", "time", "said", "think", "right", "long", "just", "probably", "years", "year", "don", "point", "need", "did", "like", "second", "didn", "tell", "know", "make", "thing", "good", "way", "say", "come", "people", "far", "going", "public", "real", "doesn", "going", "mean", "better", "far", "available", "second", "problem", "way", "don", "long", "high", "using", "year", "want", "tape", "need", "world", "know", "called", "true", "make", "right", "use", "government", "point", "probably", "got", "just", "time", "people", "think", "good", "like", "com", "key", "data", "max", "information", "new", "read", "bit", "great", "idea", "used", "high", "chip", "list", "far", "law", "case", "ve", "just", "got", "didn", "public", "number", "second", "thanks", "want", "use", "way", "called", "armenian", "said", "think", "edu", "space", "list", "mail", "different", "word", "order", "make", "tell", "thanks", "11", "right", "using", "doesn", "things", "need", "great", "world", "just", "set", "really", "like", "max", "probably", "armenian", "dod", "tape", "son", "12", "day", "idea", "believe", "people", "seen", "file", "drive", "don", "time", "drive", "chip", "hard", "set", "believe", "look", "problems", "question", "problem", "probably", "did", "idea", "help", "need", "thing", "bit", "think", "better", "thanks", "don", "case", "just", "like", "max", "way", "armenian", "dod", "tape", "son", "11", "fact", "doesn", "windows", "long", "seen", "10", "tape", "number", "following", "given", "case", "good", "government", "come", "11", "law", "second", "like", "ve", "fact", "way", "god", "new", "line", "tell", "using", "set", "world", "just", "said", "need", "don", "max", "question", "armenian", "dod", "believe", "know", "long", "10", "dod", "12", "line", "year", "ll", "years", "11", "did", "didn", "let", "things", "new", "set", "think", "know", "just", "max", "need", "armenian", "tape", "point", "son", "space", "word", "day", "called", "idea", "second", "order", "public", "drive", "hard", "game", "com", "long", "believe", "question", "information", "son", "program", "used", "use", "high", "called", "want", "probably", "help", "using", "world", "long", "thing", "need", "like", "say", "point", "try", "look", "don", "max", "fact", "armenian", "dod", "tape", "thanks", "11", "space", "word", "12", "file", "got", "line", "following", "different", "believe", "question"], "Total": [34.0, 24.0, 29.0, 38.0, 31.0, 22.0, 22.0, 39.0, 34.0, 20.0, 21.0, 17.0, 20.0, 19.0, 24.0, 18.0, 24.0, 22.0, 17.0, 28.0, 14.0, 21.0, 17.0, 37.0, 36.0, 19.0, 32.0, 55.0, 41.0, 17.0, 22.12163452636025, 21.518459700723856, 5.547173517877127, 12.822780830239385, 38.46031470494187, 24.57803366548922, 16.816308281090073, 18.603817892675128, 14.352678744202349, 15.350984526115376, 17.83151707994416, 18.21495817748294, 20.310912622086743, 13.938766680335288, 12.047305862078117, 22.390677578923473, 16.315669128528608, 18.783019188249348, 17.987371392209138, 14.231295905214939, 16.526646619116676, 12.86878776593351, 17.92011275034336, 13.048079660162978, 15.403890437510373, 25.02898952924714, 19.787308329017367, 17.85998797203906, 41.480947578594744, 18.51061036218133, 52.88845250851157, 46.989410721072865, 27.978597360955266, 37.02929956948504, 36.37249086193115, 44.25314024309225, 32.12924501018003, 55.159854489360406, 39.46892956190978, 28.987088408993834, 25.219914380064218, 21.418037693302626, 20.067572247142987, 16.406300286186386, 36.18373026621464, 36.37249086193115, 31.34604628037188, 18.51061036218133, 21.88459820289141, 16.787585226179868, 14.203375000712038, 17.92196614572182, 44.25314024309225, 17.20814313149984, 15.350984526115376, 22.607196428118876, 20.1464502604426, 23.538601169653496, 14.58219778727685, 17.92011275034336, 19.787308329017367, 29.27819217001052, 19.952842283616146, 17.7591952887798, 13.048079660162978, 55.159854489360406, 17.727632659436207, 17.64128338436025, 16.466953913544824, 38.51241056402409, 28.283731308683443, 24.57803366548922, 39.46892956190978, 37.02929956948504, 46.989410721072865, 25.219914380064218, 32.12924501018003, 34.5482675161237, 18.537056877065503, 19.95707569939026, 16.044511655156832, 17.85998797203906, 37.02929956948504, 17.987371392209138, 41.480947578594744, 27.978597360955266, 17.180323577802728, 52.88845250851157, 21.81753752121655, 21.19690465974988, 24.467778338078, 46.989410721072865, 18.783019188249348, 28.283731308683443, 28.987088408993834, 55.159854489360406, 14.13337548026207, 18.073937339892094, 18.557089738224338, 44.25314024309225, 32.12924501018003, 14.352678744202349, 39.46892956190978, 25.02898952924714, 24.57803366548922, 18.21495817748294, 38.46031470494187, 14.231295905214939, 19.99214573729844, 17.679394307440973, 14.91697596971946, 19.230183075410267, 19.99214573729844, 16.315669128528608, 16.466953913544824, 14.231295905214939, 14.203375000712038, 14.13337548026207, 23.538601169653496, 25.02898952924714, 46.989410721072865, 17.180323577802728, 15.235397498410439, 22.607196428118876, 24.467778338078, 25.219914380064218, 7.088696145043219, 28.283731308683443, 17.83151707994416, 44.25314024309225, 12.86878776593351, 16.044511655156832, 32.12924501018003, 27.978597360955266, 38.51241056402409, 20.310912622086743, 18.783019188249348, 21.81753752121655, 16.787585226179868, 52.88845250851157, 37.02929956948504, 38.46031470494187, 41.480947578594744, 39.46892956190978, 55.159854489360406, 22.73989128980392, 22.569307436816565, 14.976974073497335, 4.3275055204279385, 17.7591952887798, 34.5482675161237, 17.727632659436207, 17.20814313149984, 19.95707569939026, 13.048079660162978, 31.53019315145059, 15.235397498410439, 20.441229429708887, 14.164547972728336, 14.231295905214939, 16.816308281090073, 24.234322849377847, 31.34604628037188, 52.88845250851157, 16.787585226179868, 18.073937339892094, 17.679394307440973, 19.767726428216275, 14.13337548026207, 36.18373026621464, 25.219914380064218, 38.51241056402409, 25.02898952924714, 12.86878776593351, 5.547173517877127, 17.987371392209138, 41.480947578594744, 34.03156721263522, 11.231796477760522, 14.164547972728336, 29.27819217001052, 17.227115580786403, 12.047305862078117, 13.938766680335288, 32.12924501018003, 18.557089738224338, 36.18373026621464, 8.645743037802903, 27.978597360955266, 22.607196428118876, 19.230183075410267, 18.603817892675128, 28.283731308683443, 19.95707569939026, 17.83151707994416, 52.88845250851157, 19.952842283616146, 21.518459700723856, 55.159854489360406, 4.3275055204279385, 21.81753752121655, 5.547173517877127, 5.817300941742041, 7.088696145043219, 7.573418403224527, 12.321344931765704, 12.822780830239385, 13.048079660162978, 22.390677578923473, 38.46031470494187, 20.067572247142987, 16.406300286186386, 24.043426924897012, 46.989410721072865, 37.02929956948504, 24.043426924897012, 20.441229429708887, 17.284721165104127, 19.952842283616146, 22.390677578923473, 16.526646619116676, 17.92196614572182, 17.92011275034336, 23.538601169653496, 21.81753752121655, 28.987088408993834, 13.048079660162978, 21.88459820289141, 28.283731308683443, 14.352678744202349, 17.20814313149984, 41.480947578594744, 16.466953913544824, 36.18373026621464, 46.989410721072865, 24.234322849377847, 52.88845250851157, 55.159854489360406, 4.3275055204279385, 25.02898952924714, 5.547173517877127, 5.817300941742041, 7.088696145043219, 7.573418403224527, 8.645743037802903, 17.85998797203906, 19.230183075410267, 21.418037693302626, 17.180323577802728, 20.067572247142987, 17.134082821093486, 7.088696145043219, 19.767726428216275, 14.58219778727685, 15.403890437510373, 24.234322849377847, 39.46892956190978, 20.310912622086743, 18.21495817748294, 8.645743037802903, 16.816308281090073, 14.13337548026207, 55.159854489360406, 31.34604628037188, 17.85998797203906, 25.02898952924714, 22.12163452636025, 34.5482675161237, 21.279394353621147, 18.557089738224338, 22.607196428118876, 19.952842283616146, 17.83151707994416, 52.88845250851157, 17.987371392209138, 28.283731308683443, 46.989410721072865, 4.3275055204279385, 17.92011275034336, 5.547173517877127, 5.817300941742041, 22.390677578923473, 44.25314024309225, 17.180323577802728, 17.134082821093486, 5.817300941742041, 12.321344931765704, 21.279394353621147, 24.467778338078, 17.64128338436025, 21.19690465974988, 8.645743037802903, 28.987088408993834, 18.073937339892094, 19.787308329017367, 18.603817892675128, 34.5482675161237, 19.952842283616146, 41.480947578594744, 44.25314024309225, 52.88845250851157, 4.3275055204279385, 28.283731308683443, 5.547173517877127, 7.088696145043219, 18.783019188249348, 7.573418403224527, 11.231796477760522, 12.047305862078117, 12.822780830239385, 12.86878776593351, 13.048079660162978, 14.13337548026207, 13.938766680335288, 17.679394307440973, 24.043426924897012, 17.284721165104127, 18.537056877065503, 22.73989128980392, 17.180323577802728, 22.390677578923473, 17.92011275034336, 17.7591952887798, 7.573418403224527, 20.1464502604426, 31.53019315145059, 38.51241056402409, 15.235397498410439, 12.86878776593351, 25.219914380064218, 21.81753752121655, 21.88459820289141, 22.607196428118876, 17.83151707994416, 17.180323577802728, 14.352678744202349, 28.283731308683443, 55.159854489360406, 24.57803366548922, 18.783019188249348, 15.350984526115376, 16.526646619116676, 46.989410721072865, 4.3275055204279385, 17.85998797203906, 5.547173517877127, 5.817300941742041, 7.088696145043219, 36.18373026621464, 8.645743037802903, 11.231796477760522, 12.047305862078117, 12.321344931765704, 16.406300286186386, 16.787585226179868, 21.279394353621147, 14.58219778727685, 17.227115580786403, 22.390677578923473, 17.92011275034336], "loglift": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.186, 1.1635, 1.1388, 1.129, 1.0201, 0.8428, 0.8371, 0.7933, 0.764, 0.7352, 0.6944, 0.6876, 0.6411, 0.641, 0.6361, 0.6078, 0.6024, 0.5337, 0.4952, 0.4591, 0.4558, 0.4381, 0.4357, 0.4023, 0.3836, 0.3615, 0.3561, 0.2833, 0.2549, 0.2522, 0.2477, 0.2242, 0.2445, 0.1708, 0.1513, 0.0919, 0.0746, -0.2714, -0.1709, 0.0275, 0.0952, 1.7395, 1.6535, 1.6439, 1.402, 1.3636, 1.2764, 1.1803, 1.0956, 1.0727, 1.0606, 0.8512, 0.8305, 0.7871, 0.7362, 0.6975, 0.6546, 0.6274, 0.5214, 0.4917, 0.4123, 0.3597, 0.3268, 0.289, 0.2846, 0.2711, 0.2327, 0.2117, 0.2045, 0.1477, 0.1347, 0.0812, -0.2134, -0.2001, -0.3341, -0.0112, -0.241, -0.3154, 2.2875, 1.6167, 1.5691, 1.5647, 1.5191, 1.5019, 1.3742, 1.3695, 1.1948, 1.0052, 0.9775, 0.9103, 0.8019, 0.7878, 0.6404, 0.5752, 0.5126, 0.4961, 0.3726, 0.3646, 0.3345, 0.3242, 0.222, 0.2083, 0.1271, 0.0436, 0.0358, 0.0344, -0.0708, -0.0897, -0.1856, 2.2831, 1.9766, 1.8585, 1.7975, 1.6335, 1.4575, 1.2837, 1.2332, 1.0763, 1.0586, 1.0512, 0.8859, 0.8516, 0.8241, 0.6501, 0.5718, 0.5709, 0.5326, 0.5238, 0.4803, 0.4332, 0.3601, 0.3481, 0.303, 0.26, 0.2192, 0.1641, 0.1258, 0.0824, 0.0815, 0.0665, 0.0459, 0.0305, -0.0341, -0.0402, -0.2702, 2.6215, 2.6212, 2.5695, 2.4209, 2.1375, 1.9581, 1.8767, 1.0561, 0.965, 0.9622, 0.8202, 0.7304, 0.6139, 0.5589, 0.5426, 0.3465, 0.3146, 0.2612, 0.2504, 0.2057, -0.0092, -0.071, -0.1034, -0.2666, -0.3054, -0.5336, -1.0329, -1.1674, -1.2609, -1.2726, -1.6514, -2.0102, 2.5908, 2.4686, 2.4297, 2.3977, 2.0114, 1.8775, 1.7351, 1.6044, 1.5212, 0.9998, 0.9191, 0.7717, 0.3242, 0.1071, 0.0289, 0.0053, -0.1219, -0.2765, -0.3324, -0.397, -0.4935, -0.902, -0.9441, -1.0316, -1.192, -1.2396, -1.4374, -1.5037, -1.9902, -2.0301, -2.0473, -2.5826, -3.1268, -2.4766, -2.2754, -2.6583, -3.3284, -3.0903, 2.714, 2.5684, 2.4257, 2.2137, 1.9842, 1.7879, 1.7073, 1.2776, 1.1137, 0.9291, 0.6898, 0.657, 0.5579, 0.522, 0.5107, 0.4018, 0.3322, 0.322, -0.3702, -0.387, -0.4184, -0.5166, -0.7069, -0.863, -1.0854, -1.1113, -1.1589, -1.3564, -1.4227, -1.5547, -2.2774, -2.3533, -2.4614, -2.241, -2.3964, -2.2384, 2.4405, 2.4228, 2.2379, 2.1704, 2.1285, 1.9244, 1.7176, 1.4789, 1.4223, 1.3543, 1.1632, 1.081, 0.9374, 0.7052, 0.5898, 0.4012, 0.3221, 0.2967, 0.2867, 0.2642, 0.0062, -0.2622, -0.4888, -0.5883, -0.6856, -0.7091, -0.8774, -1.121, -1.1256, -1.1731, -1.2797, -1.8628, -2.2544, 2.7016, 2.5836, 2.5724, 2.5464, 2.186, 2.1264, 2.1011, 1.9912, 1.7979, 1.7359, 1.6396, 0.7646, 0.6995, -0.0724, -0.0797, -0.3421, -0.5123, -0.8855, -0.9221, -1.1338, -1.379, -1.4258, -1.4452, -1.8353, -1.9093, -1.9715, -1.9751, -1.9891, -2.0417, -2.055, -2.2908, -2.5989, -2.2694, -2.3396, -2.5439, -2.2638, -2.5288, -2.3061, -2.2971, 2.6802, 2.3744, 2.3328, 2.1287, 2.0723, 1.7269, 1.5242, 1.446, 1.4038, 1.3989, 1.179, 1.0092, 0.9943, 0.6241, 0.594, 0.3139, 0.1311, -0.0029, -0.1107, -0.4173, -0.8128, -0.9333, -1.0452, -1.1082, -1.3063, -1.339, -1.5044, -1.7663, -1.8366, -1.859, -2.1407, -2.1645, -2.4032, -2.0267, -2.1934, -2.4557, -2.2331], "logprob": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.3846, -3.4348, -4.8151, -3.9869, -2.9975, -3.6225, -4.0077, -3.9505, -4.2392, -4.2008, -4.0918, -4.0773, -4.0149, -4.3915, -4.5423, -3.9507, -4.2727, -4.2005, -4.2823, -4.5527, -4.4064, -4.6743, -4.3456, -4.6962, -4.549, -4.0857, -4.3261, -4.5013, -3.687, -4.4966, -3.4513, -3.593, -4.0912, -3.8846, -3.9221, -3.7854, -4.1228, -3.9283, -4.1626, -4.2728, -4.3444, -2.8634, -3.0146, -3.2256, -2.6766, -2.7097, -2.9456, -3.5686, -3.4858, -3.7739, -3.9531, -3.93, -3.0468, -4.0347, -4.1998, -3.8514, -4.0096, -3.8812, -4.466, -4.2896, -4.2699, -3.9307, -4.347, -4.5013, -4.814, -3.3859, -4.5594, -4.5852, -4.6613, -3.8685, -4.1902, -4.3841, -4.205, -4.2556, -4.1514, -4.4508, -4.4384, -4.4403, -2.4599, -3.0569, -3.3227, -3.2199, -2.5363, -3.2756, -2.5677, -2.9662, -3.6287, -2.6938, -3.607, -3.703, -3.6679, -3.0295, -4.0939, -3.7497, -3.7877, -3.1608, -4.646, -4.4082, -4.4119, -3.553, -3.9754, -4.795, -3.8645, -4.4036, -4.4296, -4.7306, -4.0884, -5.1014, -4.8575, -2.5117, -2.9881, -2.8522, -2.8744, -3.2416, -3.4083, -3.728, -3.7805, -3.9424, -3.4499, -3.3959, -2.9314, -3.9719, -4.1194, -3.8989, -3.898, -3.8686, -5.1761, -3.8011, -4.3059, -3.4441, -4.7523, -4.5437, -3.8944, -4.0758, -3.797, -4.4919, -4.6084, -4.5021, -4.765, -3.6325, -4.0096, -3.987, -3.976, -4.0319, -3.9272, -1.9215, -1.9294, -2.3912, -3.7813, -2.6528, -2.1668, -2.9154, -3.7657, -3.7086, -4.1364, -3.396, -4.2131, -4.0357, -4.4575, -4.4691, -4.4984, -4.1648, -3.9609, -3.4486, -4.6409, -4.7819, -4.8658, -4.7865, -5.2852, -4.384, -4.9731, -5.0491, -5.6145, -6.3733, -7.2265, -6.4289, -5.9522, -1.5491, -2.7799, -2.5868, -1.8926, -2.8093, -3.3008, -3.2974, -2.593, -3.2251, -3.0788, -4.5911, -3.564, -4.2247, -4.6036, -4.7149, -4.3196, -4.7955, -5.0627, -4.0314, -5.0708, -5.0917, -4.5589, -7.1463, -5.6161, -7.1459, -7.1459, -7.1461, -7.1463, -7.1461, -7.1461, -7.1458, -7.1412, -7.1444, -7.1446, -7.1449, -7.1456, -7.1457, -7.1458, -1.7734, -2.0812, -2.3917, -2.4601, -2.5744, -3.0743, -3.0739, -3.5037, -3.3948, -3.6553, -3.6105, -4.4416, -4.0235, -3.8029, -4.4926, -4.42, -3.6098, -4.5438, -4.4487, -4.2043, -4.8978, -4.2156, -4.3639, -7.0652, -5.5326, -7.0652, -7.0652, -7.0651, -7.0652, -7.0648, -7.062, -7.064, -7.0644, -7.0644, -7.0644, -7.0645, -3.2682, -2.2604, -2.7495, -2.7621, -2.3509, -2.0673, -2.9384, -3.286, -4.0878, -3.4906, -3.8555, -2.5759, -3.2847, -4.0795, -3.8574, -4.1695, -3.8027, -4.3127, -4.4597, -4.2847, -4.6676, -5.0484, -4.1878, -5.3658, -5.0105, -4.5263, -7.0796, -5.9022, -7.0795, -7.0794, -5.8383, -5.7401, -7.0778, -2.1245, -3.3227, -2.5834, -2.063, -2.2838, -2.6705, -2.5122, -3.519, -2.5024, -3.0368, -3.0426, -3.9792, -3.4254, -4.7462, -4.0217, -4.2194, -4.2113, -7.0877, -5.247, -7.0877, -7.0876, -6.16, -7.0877, -7.0838, -7.0877, -7.0875, -7.0875, -7.0876, -7.0603, -7.0876, -7.0856, -7.0862, -7.0867, -7.087, -7.087, -7.0872, -7.0873, -7.0873, -7.0873, -2.9624, -2.2898, -1.8835, -1.8875, -2.8713, -3.3855, -2.9154, -3.1385, -3.1776, -3.15, -3.6073, -3.8142, -4.009, -3.7008, -3.063, -4.1514, -4.6032, -4.9389, -4.9729, -4.2346, -7.015, -5.7179, -6.9991, -7.0146, -7.0149, -5.4176, -7.0145, -7.0148, -7.0149, -7.0149, -7.0102, -7.011, -7.0126, -7.0141, -7.0141, -7.0143, -7.0143]}, "token.table": {"Topic": [9, 6, 8, 9, 1, 9, 1, 1, 2, 4, 1, 7, 1, 2, 4, 7, 1, 2, 5, 7, 1, 2, 4, 10, 1, 4, 5, 7, 8, 5, 7, 5, 1, 2, 3, 8, 5, 1, 2, 1, 2, 3, 7, 9, 1, 2, 3, 5, 9, 1, 2, 6, 9, 1, 2, 4, 1, 4, 6, 1, 2, 3, 4, 7, 8, 10, 7, 1, 6, 1, 3, 8, 1, 3, 4, 5, 1, 2, 1, 2, 8, 3, 1, 8, 1, 8, 1, 2, 3, 4, 1, 2, 3, 4, 8, 1, 2, 4, 5, 1, 4, 8, 1, 3, 5, 6, 1, 2, 7, 1, 2, 7, 10, 1, 4, 5, 10, 1, 2, 4, 5, 7, 1, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 1, 2, 3, 4, 9, 1, 5, 8, 1, 2, 9, 1, 2, 3, 4, 6, 7, 8, 10, 4, 8, 9, 1, 5, 6, 1, 2, 9, 1, 3, 4, 10, 1, 2, 7, 10, 1, 2, 6, 1, 2, 3, 4, 6, 5, 1, 4, 1, 2, 3, 4, 6, 7, 8, 9, 10, 1, 2, 5, 8, 9, 1, 5, 8, 1, 4, 6, 1, 2, 3, 4, 1, 2, 3, 4, 10, 1, 3, 4, 6, 7, 10, 1, 2, 4, 7, 1, 2, 4, 7, 2, 10, 4, 5, 1, 2, 4, 7, 1, 2, 5, 1, 4, 1, 3, 4, 6, 1, 3, 4, 6, 1, 2, 3, 8, 1, 2, 3, 10, 1, 2, 3, 4, 5, 8, 1, 2, 2, 6, 7, 8, 9, 10, 2, 6, 4, 8, 1, 2, 3, 4, 6, 8, 1, 2, 3, 5, 6, 7, 10, 1, 3, 7, 10, 1, 2, 4, 6, 9, 1, 2, 3, 4, 7, 9, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 10, 1, 2, 4, 5, 10, 1, 2, 5, 10, 1, 2, 4, 6, 8, 10, 1, 2, 5, 8, 1, 2, 3, 4, 5, 10, 1, 2, 3, 4, 5, 7, 8, 1, 2, 1, 6, 1, 2, 4, 1, 4, 6, 8, 10, 1, 3, 4, 9, 1, 3, 9], "Freq": [0.9338112910428251, 0.11566385857497387, 0.23132771714994774, 0.4626554342998955, 0.0811599712156338, 0.811599712156338, 0.9013599419391288, 0.14081160286901787, 0.4928406100415626, 0.28162320573803573, 0.49127588753072793, 0.4466144432097527, 0.30363842798438095, 0.18218305679062857, 0.36436611358125714, 0.06072768559687619, 0.29056011225565664, 0.348672134706788, 0.174336067353394, 0.11622404490226267, 0.38853698506366635, 0.07770739701273327, 0.15541479402546654, 0.3108295880509331, 0.28884652744401756, 0.04126378963485965, 0.0825275792697193, 0.04126378963485965, 0.5364292652531755, 0.14676220969565842, 0.831652521608731, 0.9674628484202265, 0.5489993390356421, 0.054899933903564214, 0.10979986780712843, 0.27449966951782107, 0.9347682603506572, 0.8578482425636721, 0.07798620386942473, 0.2759849449908131, 0.03449811812385164, 0.1724905906192582, 0.13799247249540655, 0.379479299362368, 0.2766414371131073, 0.11065657484524291, 0.16598486226786435, 0.055328287422621455, 0.3872980119583502, 0.2902401145770773, 0.1741440687462464, 0.46438418332332376, 0.859505129625064, 0.3024263595736586, 0.6323460245631043, 0.027493305415787143, 0.31200951007441163, 0.5720174351364213, 0.052001585012401944, 0.3405022483677294, 0.10640695261491544, 0.21281390522983087, 0.21281390522983087, 0.042562781045966176, 0.021281390522983088, 0.042562781045966176, 0.956602404134972, 0.08815344827511094, 0.8815344827511095, 0.3359464748460849, 0.44792863312811326, 0.11198215828202832, 0.42160601817023213, 0.0702676696950387, 0.35133834847519346, 0.1405353393900774, 0.1219043882601576, 0.8533307178211031, 0.06857676837112389, 0.27430707348449557, 0.617190915340115, 0.9710279317462761, 0.3895119888277873, 0.5842679832416809, 0.858887709104836, 0.09040923253735117, 0.2500982168548185, 0.05001964337096369, 0.10003928674192739, 0.5502160770806006, 0.2280274661587381, 0.1266819256437434, 0.10134554051499471, 0.07600915538624603, 0.4307185471887275, 0.2978391431903268, 0.47654262910452283, 0.11913565727613071, 0.059567828638065354, 0.4923461680951587, 0.09846923361903173, 0.3446423176666111, 0.2505377077941716, 0.5010754155883432, 0.2004301662353373, 0.050107541558834325, 0.11570912720523203, 0.11570912720523203, 0.6942547632313922, 0.13708270867882041, 0.5026365984890082, 0.09138847245254694, 0.22847118113136736, 0.13127324050512412, 0.19690986075768618, 0.13127324050512412, 0.45945634176793443, 0.38319815100956756, 0.22991889060574056, 0.07663963020191351, 0.15327926040382703, 0.15327926040382703, 0.16892657303540043, 0.22523543071386723, 0.5630885767846681, 0.3403389425527847, 0.03781543806142052, 0.26470806642994366, 0.0945385951535513, 0.0945385951535513, 0.03781543806142052, 0.03781543806142052, 0.03781543806142052, 0.03781543806142052, 0.9747751481337937, 0.29376446346153373, 0.3615562627218877, 0.13558359852070787, 0.13558359852070787, 0.04519453284023596, 0.594660839516423, 0.1189321679032846, 0.2378643358065692, 0.3537621127444987, 0.25268722338892763, 0.30322466806671317, 0.19942039553642682, 0.2175495224033747, 0.16316214180253102, 0.07251650746779156, 0.01812912686694789, 0.03625825373389578, 0.18129126866947892, 0.10877476120168734, 0.09398763737181538, 0.09398763737181538, 0.7988949176604307, 0.0705987936872639, 0.1411975873745278, 0.776586730559903, 0.22674087326017167, 0.22674087326017167, 0.5101669648353863, 0.2910306070404917, 0.2910306070404917, 0.23282448563239336, 0.17461836422429503, 0.423558400038818, 0.12101668572537658, 0.3630500571761297, 0.06050834286268829, 0.03415511429781153, 0.23908580008468072, 0.7172574002540422, 0.2801186270374042, 0.12449716757217966, 0.12449716757217966, 0.12449716757217966, 0.34236721082349403, 0.6932400168731236, 0.49032619728796023, 0.49032619728796023, 0.1414240559827392, 0.17678006997842396, 0.17678006997842396, 0.1414240559827392, 0.0707120279913696, 0.10606804198705438, 0.0353560139956848, 0.0353560139956848, 0.10606804198705438, 0.1447250574190586, 0.11578004593524689, 0.49206519522479925, 0.08683503445143516, 0.11578004593524689, 0.2023500281899104, 0.0505875070474776, 0.7082250986646864, 0.5021965113940496, 0.07174235877057852, 0.3587117938528926, 0.7280231640018848, 0.0520016545715632, 0.0780024818573448, 0.1040033091431264, 0.42591661754808824, 0.15971873158053307, 0.15971873158053307, 0.10647915438702206, 0.05323957719351103, 0.1833387473774336, 0.2750081210661504, 0.0916693736887168, 0.0458346868443584, 0.1833387473774336, 0.2750081210661504, 0.21241704058634184, 0.29738385682087853, 0.25490044870361017, 0.21241704058634184, 0.16739234833986863, 0.39058214612636016, 0.05579744944662288, 0.33478469667973726, 0.2978192149205041, 0.6452749656610922, 0.9050083799118536, 0.05656302374449085, 0.3906225422530266, 0.27901610160930473, 0.05580322032186094, 0.22321288128744377, 0.28204555543622234, 0.22563644434897787, 0.45127288869795573, 0.26815086436552243, 0.6703771609138061, 0.8364910988212833, 0.04647172771229351, 0.04647172771229351, 0.04647172771229351, 0.32167445293593216, 0.3931576646994726, 0.10722481764531071, 0.14296642352708094, 0.4447564808421667, 0.055594560105270835, 0.4447564808421667, 0.055594560105270835, 0.6103010600502987, 0.16274694934674633, 0.08137347467337316, 0.08137347467337316, 0.21226351795362988, 0.07075450598454329, 0.14150901196908658, 0.28301802393817316, 0.07075450598454329, 0.21226351795362988, 0.0996632764227242, 0.8471378495931556, 0.20047269171693474, 0.050118172929233686, 0.6014180751508043, 0.050118172929233686, 0.050118172929233686, 0.9242853923163175, 0.17806590459149546, 0.8012965706617297, 0.14106966634467066, 0.7053483317233533, 0.21555104040698278, 0.1616632803052371, 0.10777552020349139, 0.053887760101745695, 0.3233265606104742, 0.10777552020349139, 0.027636730448814917, 0.663281530771558, 0.027636730448814917, 0.055273460897629835, 0.1658203826928895, 0.055273460897629835, 0.027636730448814917, 0.5573872405687013, 0.13934681014217531, 0.13934681014217531, 0.13934681014217531, 0.59127648224997, 0.16125722243181, 0.05375240747727, 0.05375240747727, 0.16125722243181, 0.3375043439756031, 0.024107453141114506, 0.3857192502578321, 0.09642981256445803, 0.09642981256445803, 0.04821490628222901, 0.3240677015097772, 0.13502820896240716, 0.43209026867970296, 0.10802256716992574, 0.24930643487141402, 0.12465321743570701, 0.43628626102497453, 0.12465321743570701, 0.5211392133442811, 0.3257120083401757, 0.06514240166803514, 0.15579393530886454, 0.1817595911936753, 0.10386262353924303, 0.025965655884810757, 0.4933474618114044, 0.1268625276345944, 0.0634312638172972, 0.15857815954324303, 0.6343126381729721, 0.044233702448667984, 0.3096359171406759, 0.17693480979467194, 0.08846740489733597, 0.08846740489733597, 0.2654022146920079, 0.15950974981909843, 0.5742350993487544, 0.09570584989145907, 0.15950974981909843, 0.2775584363416136, 0.15860482076663635, 0.039651205191659086, 0.15860482076663635, 0.039651205191659086, 0.2775584363416136, 0.3995367047604816, 0.07990734095209631, 0.11986101142814447, 0.23972202285628894, 0.039953670476048156, 0.039953670476048156, 0.11986101142814447, 0.04668961808357905, 0.933792361671581, 0.4980366621956937, 0.4150305518297448, 0.324138420214305, 0.540230700357175, 0.10804614007143501, 0.5047243013396022, 0.16824143377986742, 0.05608047792662247, 0.05608047792662247, 0.16824143377986742, 0.04087007762546832, 0.2043503881273416, 0.16348031050187328, 0.5721810867565564, 0.1887067977238899, 0.2358834971548624, 0.5189436937406973], "Term": ["10", "11", "11", "11", "12", "12", "armenian", "available", "available", "available", "believe", "believe", "better", "better", "better", "better", "bit", "bit", "bit", "bit", "called", "called", "called", "called", "case", "case", "case", "case", "case", "chip", "chip", "com", "come", "come", "come", "come", "data", "day", "day", "did", "did", "did", "did", "did", "didn", "didn", "didn", "didn", "didn", "different", "different", "different", "dod", "does", "does", "does", "doesn", "doesn", "doesn", "don", "don", "don", "don", "don", "don", "don", "drive", "edu", "edu", "fact", "fact", "fact", "far", "far", "far", "far", "file", "file", "following", "following", "following", "game", "given", "given", "god", "god", "going", "going", "going", "going", "good", "good", "good", "good", "good", "got", "got", "got", "got", "government", "government", "government", "great", "great", "great", "great", "hard", "hard", "hard", "help", "help", "help", "help", "high", "high", "high", "high", "idea", "idea", "idea", "idea", "idea", "information", "information", "information", "just", "just", "just", "just", "just", "just", "just", "just", "just", "key", "know", "know", "know", "know", "know", "law", "law", "law", "let", "let", "let", "like", "like", "like", "like", "like", "like", "like", "like", "line", "line", "line", "list", "list", "list", "ll", "ll", "ll", "long", "long", "long", "long", "look", "look", "look", "look", "mail", "mail", "mail", "make", "make", "make", "make", "make", "max", "mean", "mean", "need", "need", "need", "need", "need", "need", "need", "need", "need", "new", "new", "new", "new", "new", "number", "number", "number", "order", "order", "order", "people", "people", "people", "people", "point", "point", "point", "point", "point", "probably", "probably", "probably", "probably", "probably", "probably", "problem", "problem", "problem", "problem", "problems", "problems", "problems", "problems", "program", "program", "public", "public", "question", "question", "question", "question", "read", "read", "read", "real", "real", "really", "really", "really", "really", "right", "right", "right", "right", "said", "said", "said", "said", "say", "say", "say", "say", "second", "second", "second", "second", "second", "second", "seen", "seen", "set", "set", "set", "set", "set", "son", "space", "space", "tape", "tape", "tell", "tell", "tell", "tell", "tell", "tell", "thanks", "thanks", "thanks", "thanks", "thanks", "thanks", "thanks", "thing", "thing", "thing", "thing", "things", "things", "things", "things", "things", "think", "think", "think", "think", "think", "think", "time", "time", "time", "time", "true", "true", "true", "true", "try", "try", "try", "use", "use", "use", "use", "use", "used", "used", "used", "used", "using", "using", "using", "using", "using", "using", "ve", "ve", "ve", "ve", "want", "want", "want", "want", "want", "want", "way", "way", "way", "way", "way", "way", "way", "windows", "windows", "word", "word", "work", "work", "work", "world", "world", "world", "world", "world", "year", "year", "year", "year", "years", "years", "years"]}, "R": 30, "lambda.step": 0.01, "plot.opts": {"xlab": "PC1", "ylab": "PC2"}, "topic.order": [7, 6, 2, 10, 8, 4, 1, 5, 3, 9]};

function LDAvis_load_lib(url, callback){
  var s = document.createElement('script');
  s.src = url;
  s.async = true;
  s.onreadystatechange = s.onload = callback;
  s.onerror = function(){console.warn("failed to load library " + url);};
  document.getElementsByTagName("head")[0].appendChild(s);
}

if(typeof(LDAvis) !== "undefined"){
   // already loaded: just create the visualization
   !function(LDAvis){
       new LDAvis("#" + "ldavis_el972017619198264643308787605", ldavis_el972017619198264643308787605_data);
   }(LDAvis);
}else if(typeof define === "function" && define.amd){
   // require.js is available: use it to load d3/LDAvis
   require.config({paths: {d3: "https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min"}});
   require(["d3"], function(d3){
      window.d3 = d3;
      LDAvis_load_lib("https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js", function(){
        new LDAvis("#" + "ldavis_el972017619198264643308787605", ldavis_el972017619198264643308787605_data);
      });
    });
}else{
    // require.js not available: dynamically load d3 & LDAvis
    LDAvis_load_lib("https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js", function(){
         LDAvis_load_lib("https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js", function(){
                 new LDAvis("#" + "ldavis_el972017619198264643308787605", ldavis_el972017619198264643308787605_data);
            })
         });
}
</script>



### Another dimension reduction method for Visualization
데이터 시각화를 위한 차원축소 방법들
 
 - t-distributed Stochastic Neighbor Embedding (https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)
 - Multidimensional scaling (https://en.wikipedia.org/wiki/Multidimensional_scaling)
 - Isomap (https://en.wikipedia.org/wiki/Isomap)
 - Locally Linear Embedding (https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Locally-linear_embedding)
 
더 알아보기: https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction


```python
from sklearn.manifold import TSNE, MDS, Isomap, LocallyLinearEmbedding, SpectralEmbedding

TSNE_IRIS = TSNE(n_components=2, learning_rate=1.0, n_iter=750).fit_transform(X)
MDS_IRIS = MDS(n_components=2, dissimilarity='euclidean').fit_transform(X)
ISOMAP_IRIS = Isomap(n_neighbors=10, n_components=2).fit_transform(X)
LLE_IRIS = LocallyLinearEmbedding(n_neighbors=1, n_components=2).fit_transform(X)

plt.figure(figsize=(15,15))

plt.subplot(221)
plt.title("t-SNE")
plt.scatter(TSNE_IRIS[:,0], TSNE_IRIS[:,1], c=y)

plt.subplot(222)
plt.title("MDS")
plt.scatter(MDS_IRIS[:,0], MDS_IRIS[:,1], c=y)

plt.subplot(223)
plt.title("ISOMAP")
plt.scatter(ISOMAP_IRIS[:,0], ISOMAP_IRIS[:,1], c=y)

plt.subplot(224)
plt.title("LLE")
plt.scatter(LLE_IRIS[:,0], LLE_IRIS[:,1], c=y)

plt.show()
```


![png](/img/post_img/2018-04-13-dimensionreduction/output_25_0.png)

